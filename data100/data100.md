# `Jupyter Notebook`使用技巧

1. `ctrl+enter`只运行当前`cell`
2. `a,b,m,y`分别是在上方插入一个代码块，在下方插入，改为文本块，改为代码块的快捷键
3. 数据处理的一个常用技巧是，将要进行的步骤拆分为多个函数，并使用`pipe`函数来对`DataFrame`依次执行，这样能够避免多次运行代码块，会因为原`DataFrame`已被改变而导致错误

# `pandas`使用技巧

1. 使用`[]`来操控`DataFrame`时，一次只能对一个维度进行操控。选取一个`index`，会导致直接降维，返回的是一个`Series`，如果选取一个列表，则返回一个`DataFrame`
2. 使用`iloc,loc`来操控`DataFrame`时，一次性对所有维度进行操控，`[,]`逗号前后分别控制行列。如果出现高维，那么对应`index`为列表。可以使用`reindex`来避免高维出现

# `matplotlib`使用技巧

1. `matplotlib`会时刻记录当前的`figure`和`axes`。如果使用`plt.`等方法，实际上会自动对当前的`figure`和`axes`施加这些方法。如果使用`plt.show()`，则会打印当前的`figure`和`axes`，然后丢弃他们，将当前的`figure`和`axes`设置为全新的
2. 在`plt`会自动在每个`cell`最后调用`plt.show()`
3. `figure`,`axes`,`line`都可以保存为对象
4. `plt.setp(obj)`可以调整`obj`的一切参数
5. `plt.tight_layout()`可以使生成的图表中的子图更加紧凑
6. `seaborn`库中大多数的绘图函数，能够接受上述对象作为参数，从而修改提供的对象

# 数据处理技巧 

1. 使用`list comprehension`时，如果涉及到从字典中取值，但是可能有些时候不存在该键值对时，如果使用`[]`则会报错，此时应该使用`get`方法

# 正则表达式

1. 用`r'expression'`来存储用于匹配的`pattern`，其中`()`,`.`在不加`\`的情况下，不会被视为原文。
   1. `.`为通配符
   2. `\d`用于匹配一个数字
   3. `\w`用于匹配一个字母
   4. 以上匹配符后加上`+`，代表匹配一个或多个
   5. 用`()`括起来的部分，在匹配结束后可以调用`groups`方法来返回一个包含着匹配到的部分的`tuple`
   6. `group`方法则是返回匹配到的完整字符串
   7. `[0-9]{x}`用于匹配x个数字
   8. `(exression)*`用于匹配`expression`0次或多次
   9. `(expression1|expression2)`用于匹配`expression1`或`expression2`

# 机器学习基础

## 线性回归

1. 设计模型和损失函数
2. 根据已知数据，调整参数，使得损失函数最低

### 梯度下降

#### （batch）批梯度下降

步长为学习率乘以平均损失函数值

#### 随机梯度下降

步长为学习率乘以数据集中单个数据的损失函数值

迭代一次称为iteration，若干次迭代中遍历了整个数据集，则称为一次epoch。所以批梯度下降一次便是一个epoch，而随机梯度下降需要多次iteration才完成一次epoch

### 特征工程

特征工程即，对原有数据进行加工，得到处理后的数据，然后用其进一步进行回归

#### 多项回归

如果已有数据x，想要线性回归二次函数，就需要在数据集中加入$x^2$。（线性回归中的线性是指参数为线性）类似地，如果有多个参数，模型中的自变量为高次，则需要添加相应的项。

#### 独热编码

例如有一项为国家，有三项，则不能将三项分别转换为0，1，2.因为这些数字代表了他们的贡献，但是这应该是由学习过后的参数所决定的。常用的方法是将原先的一列分为三列，用0，1来分别标记对应行中为是否为这个国家。学习后得到的系数便是“是这个国家”这个因素对结果的贡献。为了避免和常数项线性相关，实际上必须抛弃掉其中一列，例如国家C，此时系数的含义便是“从国家C变为国家A/B”对最终结果的影响

### 正则化

正则化，即损失函数中增加一个和权重向量的模相关的惩罚项，这会限制模型的起伏。要注意的是，惩罚项中，并不包括bias项，也就是模型
\[\theta_0+\theta_1x_1+\cdots\]
中的$\theta_0$并不包含在惩罚项中

使用正则化之前，需要先对数据进行正则化，也就是中心化，并将范围限制在±1之间

#### Lasso回归

惩罚项为$\lambda\sum_{i\neq0}\lvert x_i\rvert$

当$\lambda$时，会有较多的参数为0，能够帮助我们提取出来关键的参数

#### 岭回归

惩罚项为$\lambda\sum_{i\neq0}x_i^2$

会保证所有参数非零，但是可能会特别小

## 分类

### Logistic Regression

Logistic Regression 是一种回归，用于二分类问题。它会将模型和`sigmoid`函数进行嵌套，使得得到的预测值为条件概率函数$P(Y=1|X)$。

可以根据该模型得到的条件概率函数来进一步预测类别。一般是选取一个分界点，然后根据所得到的概率和这个分界点的比较大小，来进行分类

预测的结果可以用准确率、错误率、预测率、召回率来评价。`false positive rate - true positive rate`图称为ROC图，是根据不同的分界点来所得到的坐标点绘制而成的。可以根据ROC曲线以及实际背景来选取合适的分界点

ROC曲线下的面积也可以用来衡量模型的质量，当是随机分类时，面积为1/2

## bootstrap

思想是，从整个样本中，再通过有放回抽样，生成第二层样本，用于模拟总体。然后在这第二层样本中，抽取若干次样本，在其中计算估计量，通过这些样本上得到的估计量，来获取估计量的方差，均值等，主要用于获取置信区间。如果线性回归中，一个参数通过这样的方式得到的置信区间包括0，那么说明这个参数不太有用