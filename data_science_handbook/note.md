# Data Science Handbook

## 机器学习基础

### 编程技巧

- `sklearn`中的模型大多数要求`X`参数是二维的，`y`参数是一维的，所以即使当特征只有一个时，也需要将一维数组调整维只有一列的数组然后再传入
- `np.array[:,np.newaxis]`可以实现升维
- `np.array.reval()`可以实现降维
- 对于时间序列数据，将csv导入dataframe时，可以使用`parse_date`参数，并将`index`设为`Date`
- 对于时间序列数据，可以用`resample('D')`等来代替`groupby`方法，用于按照时间来聚类
- 用`np.meshgrid`创建二维网格，输入横纵坐标，返回的是两个二维数组，分别记录网格中每个点的横纵坐标。通常在计算二维平面上的函数值时，会把返回的两个数组通过`ravel`方法展开维1维数组

### 可视化技巧

- 可以用`x = plt.axis(),plt.axis(x)`来实现保存当前坐标轴的范围以及回复坐标轴范围
- 可以使用`plt.fill_between`来实现两个曲线间的填充

### 模型评估

1. 学习率曲线即准确率关于数据集大小的曲线
2. 对于线性回归模型，可以用`bootstrap`方法来评估各个参数的`std`
3. 对于`probabilistic classification`，可以调用方法`predict_proba`来获得数据是各个类别的概率，然后手动设定分界点

## 朴素贝叶斯模型

称为`generative classification`

使用场景

- 数据符合模型假设（很少见）
- 很分散的类别
- 维度很高的数据

### 高斯朴素贝叶斯

- 用于分类问题
- 假设各个类别的数据相对于各个特征的分布是独立的，且服从高斯分布
- 输入各个特征，得到数据是各个类别的概率
- 很简单，可以作为基准线

### 多项式朴素贝叶斯

- 适用于特征为出现的次数或比例的数据

## 支持向量机

- 称为`discrimative classification`
- 对离群点不敏感，决定模型的只有在分类边界的几个点
- 参数：
  - Kernel：用于特征工程的函数，使得可以拟合非线性数据
  - C：分解的柔化程度，数值越高，越硬，使得两种类别之间有明确的划分，分界线之间的数据越少
- 适合超高维数据，甚至特征数量高于样本量的情况
- 时间复杂度高
- 对参数敏感
- 解释性差

## 随机森林

- 是`ensemble model`
- 本质上是`probabilistic classification`，根据每个数给出的分类结果给出数据属于每个标签的概率
- 可解释性差

## 随机森林回归

- 不需要先验知识便能拟合非线性数据

## PCA 

- 可以用于：
  - 展现数据之间的关系
  - 理解数据中主要的差异在哪个方面
  - 展现数据内部的实际维度
- 易受离群值影响
  - 可以用Randomized PCA或Sparse PCA解决
- 不适合非线性的数据

## 流形学习

- 可以提取出镶嵌在高维空间中的低维非线性数据
- 用处较窄，且缺点较多
- 当PCA无法有效降维，即所需要保留的主成分较多时，可以使用流形学习代替

## K-means

- 缺点有：
  - 没法度量分类的不确定性
  - 分类的依据是落在以中心点为圆心的圆内，没法改变为其他形状

## GMM

- 是 probabilistic classification，可以通过参数来调整区域的形状：圆形，和坐标轴平行的椭圆，可旋转的椭圆，从而克服了PCA的问题
- 主要还是用于拟合数据的分布，从而可以使用该模型生成更多和原始数据分布类似的数据
  - 可以用AIC或BIC曲线来评估拟合分布时，类簇数选取的好坏
  - 原始数据维度不宜过高，否则耗时较多